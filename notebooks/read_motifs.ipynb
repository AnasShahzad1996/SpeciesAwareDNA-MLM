{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import gc\n",
    "import pysam\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import helpers.train_eval as train_eval    #train and evaluation\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "\n",
    "import encoding_utils.sequence_encoders as sequence_encoders\n",
    "import encoding_utils.sequence_utils as sequence_utils\n",
    "from models.spec_dss import DSSResNet, DSSResNetEmb, SpecAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fasta_fa, seq_df, transform, motifs):\n",
    "        \n",
    "        self.fasta = pysam.FastaFile(fasta_fa)\n",
    "        \n",
    "        self.seq_df = seq_df\n",
    "        self.transform = transform\n",
    "\n",
    "        self.motifs = motifs\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.seq_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        #print(seq)\n",
    "                \n",
    "        species_label = self.seq_df.iloc[idx].species_label\n",
    "        #print(species_label)\n",
    "        # x_batch, y_masked_batch, y_batch, mask_batch, motif_mask_batch \n",
    "        masked_sequence, target_labels_masked, target_labels, mask, motif_mask_batch = self.transform(seq, motifs = self.motifs)\n",
    "        \n",
    "        masked_sequence = (masked_sequence, species_label)\n",
    "        return masked_sequence, target_labels_masked, target_labels, motif_mask_batch\n",
    "    \n",
    "    def close(self):\n",
    "        self.fasta.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading sequences and filtering motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>species_name</th>\n",
       "      <th>species_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000641515.2_utr3_2_0_chr1_70009_f:Homo_s...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000616016.5_utr3_13_0_chr1_944154_f:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000327044.7_utr3_18_0_chr1_944203_r:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000338591.8_utr3_11_0_chr1_965192_f:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000379410.8_utr3_15_0_chr1_974576_f:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18129</th>\n",
       "      <td>ENST00000303766.12_utr3_11_0_chrY_22168542_r:H...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18130</th>\n",
       "      <td>ENST00000250831.6_utr3_11_0_chrY_22417604_f:Ho...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18131</th>\n",
       "      <td>ENST00000303728.5_utr3_4_0_chrY_22514071_f:Hom...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18132</th>\n",
       "      <td>ENST00000382407.1_utr3_0_0_chrY_24045793_r:Hom...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18133</th>\n",
       "      <td>ENST00000306609.5_utr3_1_0_chrY_25624528_f:Hom...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18134 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                seq_name  species_name  \\\n",
       "0      ENST00000641515.2_utr3_2_0_chr1_70009_f:Homo_s...  Homo_sapiens   \n",
       "1      ENST00000616016.5_utr3_13_0_chr1_944154_f:Homo...  Homo_sapiens   \n",
       "2      ENST00000327044.7_utr3_18_0_chr1_944203_r:Homo...  Homo_sapiens   \n",
       "3      ENST00000338591.8_utr3_11_0_chr1_965192_f:Homo...  Homo_sapiens   \n",
       "4      ENST00000379410.8_utr3_15_0_chr1_974576_f:Homo...  Homo_sapiens   \n",
       "...                                                  ...           ...   \n",
       "18129  ENST00000303766.12_utr3_11_0_chrY_22168542_r:H...  Homo_sapiens   \n",
       "18130  ENST00000250831.6_utr3_11_0_chrY_22417604_f:Ho...  Homo_sapiens   \n",
       "18131  ENST00000303728.5_utr3_4_0_chrY_22514071_f:Hom...  Homo_sapiens   \n",
       "18132  ENST00000382407.1_utr3_0_0_chrY_24045793_r:Hom...  Homo_sapiens   \n",
       "18133  ENST00000306609.5_utr3_1_0_chrY_25624528_f:Hom...  Homo_sapiens   \n",
       "\n",
       "       species_label  \n",
       "0                181  \n",
       "1                181  \n",
       "2                181  \n",
       "3                181  \n",
       "4                181  \n",
       "...              ...  \n",
       "18129            181  \n",
       "18130            181  \n",
       "18131            181  \n",
       "18132            181  \n",
       "18133            181  \n",
       "\n",
       "[18134 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_fa = \"../../test/Homo_sapiens_3prime_UTR.fa\"\n",
    "species_list = \"../240_species.txt\"\n",
    "\n",
    "seq_df = pd.read_csv(fasta_fa + '.fai', header=None, sep='\\t', usecols=[0], names=['seq_name'])\n",
    "seq_df['species_name'] = seq_df.seq_name.apply(lambda x:x.split(':')[1])\n",
    "species_encoding = pd.read_csv(species_list, header=None).squeeze().to_dict()\n",
    "species_encoding = {species:idx for idx,species in species_encoding.items()}\n",
    "species_encoding['Homo_sapiens'] = species_encoding['Pan_troglodytes']\n",
    "seq_df['species_label'] = seq_df.species_name.map(species_encoding)\n",
    "seq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCCCC',\n",
       " 'TATGT',\n",
       " 'GGGGG',\n",
       " 'GCATG',\n",
       " 'TTTTT',\n",
       " 'AAAAA',\n",
       " 'TTTCT',\n",
       " 'ATAAA',\n",
       " 'TATAT',\n",
       " 'ACACA',\n",
       " 'TGTAT',\n",
       " 'GAAGA',\n",
       " 'GTATG']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_overlap = [\n",
    "    (\"EWSR1\",\"GGGGG\"),\n",
    "    (\"FUS\", \"GGGGG\"),\n",
    "    (\"TAF15\", \"GGGGG\"),\n",
    "    (\"HNRNPL\", \"ACACA\"),\n",
    "    (\"PABPN1L\", \"AAAAA\"),\n",
    "    (\"TRA2A\", \"GAAGA\"),\n",
    "    (\"PCBP2\", \"CCCCC\"),\n",
    "    (\"RBFOX2\", \"GCATG\"),\n",
    "    (\"TARDBP\", \"GTATG\"),\n",
    "    (\"HNRNPC\", \"TTTTT\"),\n",
    "    (\"TIA1\",\"TTTTT\"),\n",
    "    (\"PTBP3\", \"TTTCT\"),\n",
    "    (\"CELF1\", \"TATGT\"),\n",
    "    (\"FUBP3\", \"TATAT\"),\n",
    "    (\"KHSRP\", \"TGTAT\"),\n",
    "    (\"PUM1\", \"TGTAT\"),\n",
    "    (\"KHDRBS2\", \"ATAAA\")\n",
    "]\n",
    "\n",
    "motifs = list(set(map(lambda x: x[1], motif_overlap)))\n",
    "#motifs = dict(zip(motifs, range(len(motifs))))\n",
    "motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kseq_len = 5000\n",
    "total_len = 5000\n",
    "\n",
    "seq_transform = sequence_encoders.RollingMasker()\n",
    "# motif: motif_id\n",
    "motifs = {\"AAAAA\":1, \"ATAAA\":2}\n",
    "                       \n",
    "test_dataset = SeqDataset(fasta_fa, seq_df, transform = seq_transform, motifs=motifs)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 1, collate_fn = None, shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cpu')\n",
    "d_model = 128\n",
    "n_layers = 4\n",
    "dropout = 0.\n",
    "learn_rate = 1e-4\n",
    "weight_decay = 0.\n",
    "output_dir = \"./test/\"\n",
    "get_embeddings = True\n",
    "save_at = None\n",
    "\n",
    "species_encoder = SpecAdd(embed = True, encoder = 'label', d_model = 128)\n",
    "\n",
    "model = DSSResNetEmb(d_input = 5, d_output = 5, d_model = d_model, n_layers = n_layers, \n",
    "                     dropout = dropout, embed_before = True, species_encoder = species_encoder)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_params, lr = learn_rate, weight_decay = weight_decay)\n",
    "\n",
    "last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_weight = \"../../test/MLM_mammals_species_aware_5000_weights\"\n",
    "model.load_state_dict(torch.load(model_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_dir = os.path.join(output_dir, 'predictions') #dir to save predictions\n",
    "weights_dir = os.path.join(output_dir, 'weights') #dir to save model weights at save_at epochs\n",
    "if save_at:\n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "def metrics_to_str(metrics):\n",
    "    loss, total_acc, masked_acc = metrics\n",
    "    return f'loss: {loss:.4}, total acc: {total_acc:.3f}, masked acc: {masked_acc:.3f}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from helpers.metrics import MaskedAccuracy\n",
    "def model_eval_check(model, optimizer, dataloader, device, get_embeddings = False, silent=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "\n",
    "    metric = MaskedAccuracy().to(device)\n",
    "    motif_metric = MaskedAccuracy().to(device)\n",
    "\n",
    "    model.eval() #model to train mode\n",
    "\n",
    "    if not silent:\n",
    "        tot_itr = len(dataloader.dataset)//dataloader.batch_size #total train iterations\n",
    "        pbar = tqdm(total = tot_itr, ncols=700) #progress bar\n",
    "\n",
    "    avg_loss, masked_acc, total_acc = 0., 0., 0.\n",
    "    \n",
    "    all_embeddings = []\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        #               x_batch, y_masked_batch, y_batch, mask_batch, motif_mask_batch\n",
    "        for itr_idx, (((masked_sequence, species_label), targets_masked, targets, motif_mask)) in enumerate(dataloader):\n",
    "            \n",
    "            if get_embeddings:\n",
    "                #batches are generated by transformation in the dataset,\n",
    "                #so remove extra batch dimension added by dataloader\n",
    "                masked_sequence, targets_masked, targets = masked_sequence[0], targets_masked[0], targets[0]\n",
    "                species_label = species_label.tile((len(masked_sequence),))\n",
    "            \n",
    "            masked_sequence = masked_sequence.to(device)\n",
    "            targets_masked = targets_masked.to(device)\n",
    "            motif_targets=targets.detach().clone()\n",
    "            #print(motif_targets)\n",
    "            print(targets.shape)\n",
    "            print(motif_mask.shape)\n",
    "            motif_targets[motif_mask.squeeze() == 0] = -100.0\n",
    "            motif_targets[targets_masked == -100] = -100.0\n",
    "            targets = targets.to(device)\n",
    "            species_label = torch.tensor(species_label).long().to(device)\n",
    "            \n",
    "            logits, embeddings = model(masked_sequence, species_label)\n",
    "\n",
    "            loss = criterion(logits, targets_masked)\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "\n",
    "            test_acc_motif = motif_metric(preds, motif_targets)\n",
    "            masked_acc += metric(preds, targets_masked).detach() # compute only on masked nucleotides\n",
    "            total_acc += metric(preds, targets).detach()\n",
    "            print(masked_acc/(itr_idx+1))\n",
    "                \n",
    "            if get_embeddings:\n",
    "                # only get embeddings of the masked nucleotide\n",
    "                sequence_embedding = embeddings[\"seq_embedding\"]\n",
    "                sequence_embedding = sequence_embedding.transpose(-1,-2)[targets_masked!=-100]\n",
    "                # shape # B, L, dim  to L,dim, left with only masked nucleotide embeddings\n",
    "                # average over sequence \n",
    "                #print(sequence_embedding.shape)\n",
    "                sequence_embedding = sequence_embedding.mean(dim=0) # if we mask\n",
    "                #sequence_embedding = sequence_embedding[0].mean(dim=-1) # no mask\n",
    "\n",
    "                sequence_embedding = sequence_embedding.detach().cpu().numpy()\n",
    "                all_embeddings.append(sequence_embedding)\n",
    "                \n",
    "            if not silent:\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"acc: {total_acc/(itr_idx+1):.2}, masked acc: {masked_acc/(itr_idx+1):.2}, motif acc {test_acc_motif/(itr_idx+1):.2} loss: {avg_loss/(itr_idx+1):.4}\")\n",
    "            if itr_idx == 2:\n",
    "                break\n",
    "            outputs.append({\"loss\": loss, \"preds\": preds, \"logits\": logits, \"targets\": targets_masked, \"motifs\": motif_mask})\n",
    "    if not silent:\n",
    "        del pbar\n",
    "    return outputs\n",
    "    #return (avg_loss/(itr_idx+1), total_acc/(itr_idx+1), masked_acc/(itr_idx+1)), all_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1577])\n",
      "torch.Size([1, 30, 1577])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6017/526996393.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  species_label = torch.tensor(species_label).long().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3989)\n",
      "torch.Size([30, 421])\n",
      "torch.Size([1, 30, 421])\n",
      "tensor(0.4726)\n",
      "torch.Size([30, 491])\n",
      "torch.Size([1, 30, 491])\n",
      "tensor(0.4895)\n",
      "0.48953282833099365\n"
     ]
    }
   ],
   "source": [
    "outputs = model_eval_check(model, optimizer, test_dataloader, device, \n",
    "                                                        get_embeddings = get_embeddings, silent = True)\n",
    "print(float(test_metrics[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023/06/08-15:25:43]- EPOCH 0: Test/Inference...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEPOCH \u001b[39m\u001b[39m{\u001b[39;00mlast_epoch\u001b[39m}\u001b[39;00m\u001b[39m: Test/Inference...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m test_metrics, test_embeddings \u001b[39m=\u001b[39m  train_eval\u001b[39m.\u001b[39;49mmodel_eval(model, optimizer, test_dataloader, device, \n\u001b[1;32m      4\u001b[0m                                                         get_embeddings \u001b[39m=\u001b[39;49m get_embeddings, silent \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mlast_epoch\u001b[39m}\u001b[39;00m\u001b[39m - test, \u001b[39m\u001b[39m{\u001b[39;00mmetrics_to_str(test_metrics)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m get_embeddings:   \n",
      "File \u001b[0;32m~/Projects/ML4RG-2023-project/notebooks/../helpers/train_eval.py:85\u001b[0m, in \u001b[0;36mmodel_eval\u001b[0;34m(model, optimizer, dataloader, device, get_embeddings, silent)\u001b[0m\n\u001b[1;32m     81\u001b[0m all_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mfor\u001b[39;00m itr_idx, (((masked_sequence, species_label), targets_masked, targets)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     87\u001b[0m         \u001b[39mif\u001b[39;00m get_embeddings:\n\u001b[1;32m     88\u001b[0m             \u001b[39m#batches are generated by transformation in the dataset,\u001b[39;00m\n\u001b[1;32m     89\u001b[0m             \u001b[39m#so remove extra batch dimension added by dataloader\u001b[39;00m\n\u001b[1;32m     90\u001b[0m             masked_sequence, targets_masked, targets \u001b[39m=\u001b[39m masked_sequence[\u001b[39m0\u001b[39m], targets_masked[\u001b[39m0\u001b[39m], targets[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'EPOCH {last_epoch}: Test/Inference...')\n",
    "\n",
    "test_metrics, test_embeddings =  train_eval.model_eval(model, optimizer, test_dataloader, device, \n",
    "                                                        get_embeddings = get_embeddings, silent = True)\n",
    "\n",
    "\n",
    "\n",
    "print(f'epoch {last_epoch} - test, {metrics_to_str(test_metrics)}')\n",
    "\n",
    "if get_embeddings:   \n",
    "    print(f'EPOCH {last_epoch}: Test/Inference...')\n",
    "\n",
    "test_metrics, test_embeddings =  train_eval.model_eval(model, optimizer, test_dataloader, device, \n",
    "                                                        get_embeddings = get_embeddings, silent = True)\n",
    "\n",
    "print(f'epoch {last_epoch} - test, {metrics_to_str(test_metrics)}')\n",
    "\n",
    "if get_embeddings:\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    with open(output_dir + '/embeddings.npy', 'wb') as f:\n",
    "        test_embeddings = np.vstack(test_embeddings)\n",
    "        np.save(f, test_embeddings)\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    with open(output_dir + '/embeddings.npy', 'wb') as f:\n",
    "        test_embeddings = np.vstack(test_embeddings)\n",
    "        np.save(f, test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('ML4RG-mlm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7231443d2c6613b194813e6f98d913231f722dc8bb3ac4a1397dcf3c267e4542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
