{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import gc\n",
    "import pysam\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import helpers.train_eval as train_eval    #train and evaluation\n",
    "import helpers.misc as misc                #miscellaneous functions\n",
    "from helpers.plots import MetricsHandler, MotifMetrics\n",
    "\n",
    "import encoding_utils.sequence_encoders as sequence_encoders\n",
    "import encoding_utils.sequence_utils as sequence_utils\n",
    "from models.spec_dss import DSSResNet, DSSResNetEmb, SpecAdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SeqDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, fasta_fa, seq_df, transform, motifs):\n",
    "        \n",
    "        self.fasta = pysam.FastaFile(fasta_fa)\n",
    "        \n",
    "        self.seq_df = seq_df\n",
    "        self.transform = transform\n",
    "\n",
    "        self.motifs = motifs\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.seq_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        seq = self.fasta.fetch(self.seq_df.iloc[idx].seq_name).upper()\n",
    "        #print(seq)\n",
    "                \n",
    "        species_label = self.seq_df.iloc[idx].species_label\n",
    "        #print(species_label)\n",
    "        # x_batch, y_masked_batch, y_batch, mask_batch, motif_mask_batch \n",
    "        masked_sequence, target_labels_masked, target_labels, mask, motif_mask_batch = self.transform(seq, motifs = self.motifs)\n",
    "        \n",
    "        masked_sequence = (masked_sequence, species_label)\n",
    "        return masked_sequence, target_labels_masked, target_labels, motif_mask_batch\n",
    "    \n",
    "    def close(self):\n",
    "        self.fasta.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading sequences and filtering motifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_name</th>\n",
       "      <th>species_name</th>\n",
       "      <th>species_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000641515.2_utr3_2_0_chr1_70009_f:Homo_s...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000616016.5_utr3_13_0_chr1_944154_f:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000327044.7_utr3_18_0_chr1_944203_r:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000338591.8_utr3_11_0_chr1_965192_f:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000379410.8_utr3_15_0_chr1_974576_f:Homo...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18129</th>\n",
       "      <td>ENST00000303766.12_utr3_11_0_chrY_22168542_r:H...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18130</th>\n",
       "      <td>ENST00000250831.6_utr3_11_0_chrY_22417604_f:Ho...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18131</th>\n",
       "      <td>ENST00000303728.5_utr3_4_0_chrY_22514071_f:Hom...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18132</th>\n",
       "      <td>ENST00000382407.1_utr3_0_0_chrY_24045793_r:Hom...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18133</th>\n",
       "      <td>ENST00000306609.5_utr3_1_0_chrY_25624528_f:Hom...</td>\n",
       "      <td>Homo_sapiens</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18134 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                seq_name  species_name  \\\n",
       "0      ENST00000641515.2_utr3_2_0_chr1_70009_f:Homo_s...  Homo_sapiens   \n",
       "1      ENST00000616016.5_utr3_13_0_chr1_944154_f:Homo...  Homo_sapiens   \n",
       "2      ENST00000327044.7_utr3_18_0_chr1_944203_r:Homo...  Homo_sapiens   \n",
       "3      ENST00000338591.8_utr3_11_0_chr1_965192_f:Homo...  Homo_sapiens   \n",
       "4      ENST00000379410.8_utr3_15_0_chr1_974576_f:Homo...  Homo_sapiens   \n",
       "...                                                  ...           ...   \n",
       "18129  ENST00000303766.12_utr3_11_0_chrY_22168542_r:H...  Homo_sapiens   \n",
       "18130  ENST00000250831.6_utr3_11_0_chrY_22417604_f:Ho...  Homo_sapiens   \n",
       "18131  ENST00000303728.5_utr3_4_0_chrY_22514071_f:Hom...  Homo_sapiens   \n",
       "18132  ENST00000382407.1_utr3_0_0_chrY_24045793_r:Hom...  Homo_sapiens   \n",
       "18133  ENST00000306609.5_utr3_1_0_chrY_25624528_f:Hom...  Homo_sapiens   \n",
       "\n",
       "       species_label  \n",
       "0                181  \n",
       "1                181  \n",
       "2                181  \n",
       "3                181  \n",
       "4                181  \n",
       "...              ...  \n",
       "18129            181  \n",
       "18130            181  \n",
       "18131            181  \n",
       "18132            181  \n",
       "18133            181  \n",
       "\n",
       "[18134 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta_fa = \"../../test/Homo_sapiens_3prime_UTR.fa\"\n",
    "species_list = \"../240_species.txt\"\n",
    "\n",
    "seq_df = pd.read_csv(fasta_fa + '.fai', header=None, sep='\\t', usecols=[0], names=['seq_name'])\n",
    "seq_df['species_name'] = seq_df.seq_name.apply(lambda x:x.split(':')[1])\n",
    "species_encoding = pd.read_csv(species_list, header=None).squeeze().to_dict()\n",
    "species_encoding = {species:idx for idx,species in species_encoding.items()}\n",
    "species_encoding['Homo_sapiens'] = species_encoding['Pan_troglodytes']\n",
    "seq_df['species_label'] = seq_df.species_name.map(species_encoding)\n",
    "seq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GGGGG': 7,\n",
       " 'ACACA': 4,\n",
       " 'AAAAA': 0,\n",
       " 'GAAGA': 1,\n",
       " 'CCCCC': 12,\n",
       " 'GCATG': 11,\n",
       " 'GTATG': 9,\n",
       " 'TTTTT': 2,\n",
       " 'TTTCT': 10,\n",
       " 'TATGT': 3,\n",
       " 'TATAT': 8,\n",
       " 'TGTAT': 6,\n",
       " 'ATAAA': 5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.motifs import MotifHandler\n",
    "\n",
    "motif_overlap = [\n",
    "    (\"EWSR1\",\"GGGGG\"),\n",
    "    (\"FUS\", \"GGGGG\"),\n",
    "    (\"TAF15\", \"GGGGG\"),\n",
    "    (\"HNRNPL\", \"ACACA\"),\n",
    "    (\"PABPN1L\", \"AAAAA\"),\n",
    "    (\"TRA2A\", \"GAAGA\"),\n",
    "    (\"PCBP2\", \"CCCCC\"),\n",
    "    (\"RBFOX2\", \"GCATG\"),\n",
    "    (\"TARDBP\", \"GTATG\"),\n",
    "    (\"HNRNPC\", \"TTTTT\"),\n",
    "    (\"TIA1\",\"TTTTT\"),\n",
    "    (\"PTBP3\", \"TTTCT\"),\n",
    "    (\"CELF1\", \"TATGT\"),\n",
    "    (\"FUBP3\", \"TATAT\"),\n",
    "    (\"KHSRP\", \"TGTAT\"),\n",
    "    (\"PUM1\", \"TGTAT\"),\n",
    "    (\"KHDRBS2\", \"ATAAA\")\n",
    "]\n",
    "\n",
    "# create a motif to id mapping, ids can overlap but proteins should not\n",
    "motifs = list(set(map(lambda x: x[1], motif_overlap)))\n",
    "motifs = dict(zip(motifs, range(len(motifs)))) # (motif, id)\n",
    "\n",
    "#now add ids to the motif_overlap\n",
    "motif_overlap = list(map(lambda x: (x[0], x[1], motifs[x[1]]), motif_overlap))\n",
    "\n",
    "# MotifHandler takes a list of tuples \n",
    "# (protein, motif, id, motif_regex)\n",
    "motifs = MotifHandler(motif_overlap)\n",
    "motifs.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kseq_len = 5000\n",
    "total_len = 5000\n",
    "\n",
    "seq_transform = sequence_encoders.RollingMasker()\n",
    "                       \n",
    "test_dataset = SeqDataset(fasta_fa, seq_df, transform = seq_transform, motifs=motifs.dict)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = 1, num_workers = 1, collate_fn = None, shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# test wether cuda is available - if cpu_bool is set to True, cuda is not used\n",
    "\n",
    "cpu_bool = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu_bool else \"cpu\")\n",
    "\n",
    "d_model = 128\n",
    "n_layers = 4\n",
    "dropout = 0.\n",
    "learn_rate = 1e-4\n",
    "weight_decay = 0.\n",
    "output_dir = \"./test/\"\n",
    "get_embeddings = True\n",
    "save_at = None\n",
    "\n",
    "species_encoder = SpecAdd(embed = True, encoder = 'label', d_model = 128)\n",
    "\n",
    "model = DSSResNetEmb(d_input = 5, d_output = 5, d_model = d_model, n_layers = n_layers, \n",
    "                     dropout = dropout, embed_before = True, species_encoder = species_encoder)\n",
    "\n",
    "model = model.to(device) \n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(model_params, lr = learn_rate, weight_decay = weight_decay)\n",
    "\n",
    "last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_weight = \"../../test/MLM_mammals_species_aware_5000_weights\"\n",
    "# load model but avoid torch._C._cuda_getDeviceCount() > 0 failed error\n",
    "model.load_state_dict(torch.load(model_weight, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_dir = os.path.join(output_dir, 'predictions') #dir to save predictions\n",
    "weights_dir = os.path.join(output_dir, 'weights') #dir to save model weights at save_at epochs\n",
    "if save_at:\n",
    "    os.makedirs(weights_dir, exist_ok = True)\n",
    "\n",
    "def metrics_to_str(metrics):\n",
    "    loss, total_acc, masked_acc = metrics\n",
    "    return f'loss: {loss:.4}, total acc: {total_acc:.3f}, masked acc: {masked_acc:.3f}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from helpers.metrics import MaskedAccuracy\n",
    "def model_eval_check(model, optimizer, dataloader, device, get_embeddings = False, silent=False):\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction = \"mean\")\n",
    "\n",
    "    metric = MaskedAccuracy().to(device)\n",
    "    motif_metric = MaskedAccuracy().to(device)\n",
    "\n",
    "    model.eval() #model to train mode\n",
    "\n",
    "    if not silent:\n",
    "        tot_itr = len(dataloader.dataset)//dataloader.batch_size #total train iterations\n",
    "        pbar = tqdm(total = tot_itr, ncols=700) #progress bar\n",
    "\n",
    "    avg_loss, masked_acc, total_acc = 0., 0., 0.\n",
    "    \n",
    "    all_embeddings = []\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        #               x_batch, y_masked_batch, y_batch, mask_batch, motif_mask_batch\n",
    "        for itr_idx, (((masked_sequence, species_label), targets_masked, targets, motif_mask)) in enumerate(dataloader):\n",
    "            \n",
    "            if get_embeddings:\n",
    "                #batches are generated by transformation in the dataset,\n",
    "                #so remove extra batch dimension added by dataloader\n",
    "                masked_sequence, targets_masked, targets = masked_sequence[0], targets_masked[0], targets[0]\n",
    "                species_label = species_label.tile((len(masked_sequence),))\n",
    "            \n",
    "            masked_sequence = masked_sequence.to(device)\n",
    "            targets_masked = targets_masked.to(device)\n",
    "\n",
    "            motif_targets=targets.detach().clone()\n",
    "            motif_targets[motif_mask.squeeze() == 0] = -100.0\n",
    "            motif_targets[targets_masked == -100] = -100.0\n",
    "            targets = targets.to(device)\n",
    "            species_label = torch.tensor(species_label).long().to(device)\n",
    "            \n",
    "            logits, embeddings = model(masked_sequence, species_label)\n",
    "\n",
    "            loss = criterion(logits, targets_masked)\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "\n",
    "            test_acc_motif = motif_metric(preds, motif_targets)\n",
    "            masked_acc += metric(preds, targets_masked).detach() # compute only on masked nucleotides\n",
    "            total_acc += metric(preds, targets).detach()\n",
    "            #print(masked_acc/(itr_idx+1))\n",
    "                \n",
    "            if get_embeddings:\n",
    "                # only get embeddings of the masked nucleotide\n",
    "                sequence_embedding = embeddings[\"seq_embedding\"]\n",
    "                sequence_embedding = sequence_embedding.transpose(-1,-2)[targets_masked!=-100]\n",
    "                # shape # B, L, dim  to L,dim, left with only masked nucleotide embeddings\n",
    "                # average over sequence \n",
    "                #print(sequence_embedding.shape)\n",
    "                sequence_embedding = sequence_embedding.mean(dim=0) # if we mask\n",
    "                #sequence_embedding = sequence_embedding[0].mean(dim=-1) # no mask\n",
    "\n",
    "                sequence_embedding = sequence_embedding.detach().cpu().numpy()\n",
    "                all_embeddings.append(sequence_embedding)\n",
    "                \n",
    "            if not silent:\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"acc: {total_acc/(itr_idx+1):.2}, masked acc: {masked_acc/(itr_idx+1):.2}, motif acc {test_acc_motif/(itr_idx+1):.2} loss: {avg_loss/(itr_idx+1):.4}\")\n",
    "            if itr_idx == 2:\n",
    "                break\n",
    "            outputs.append({\"loss\": loss, \"preds\": preds, \"logits\": logits, \"targets\": targets_masked, \"motifs\": motif_mask})\n",
    "    if not silent:\n",
    "        del pbar\n",
    "    return outputs\n",
    "    #return (avg_loss/(itr_idx+1), total_acc/(itr_idx+1), masked_acc/(itr_idx+1)), all_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118226/875628294.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  species_label = torch.tensor(species_label).long().to(device)\n",
      "/home/lukas/Projects/ML4RG-2023-project/notebooks/../models/dss.py:335: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1680527322149/work/aten/src/ATen/native/Copy.cpp:276.)\n",
      "  return einsum('chn,hnl->chl', W, S).float(), state                   # [C H L]\n",
      "/home/lukas/.local/lib/anaconda3/envs/ML4RG-mlm/lib/python3.9/site-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
      "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3989)\n",
      "tensor(0.4726)\n",
      "tensor(0.4895)\n"
     ]
    }
   ],
   "source": [
    "outputs = model_eval_check(model, optimizer, test_dataloader, device, \n",
    "                                                        get_embeddings = get_embeddings, silent = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the .pt files and writes them to the cwd\n",
    "# motif dict can be gotten from the motif handler\n",
    "motif_metrics = MotifMetrics(outputs=outputs, plots_dir=\"../test_dir/\", motif_dict=motifs.dict, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('ML4RG-mlm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7231443d2c6613b194813e6f98d913231f722dc8bb3ac4a1397dcf3c267e4542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
